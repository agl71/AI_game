{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "13eb3394-675f-4a5d-adbb-ea2c3460f587",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, AdamW\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1482ea37-b849-4c1c-93d4-412334559380",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f813410f5d0>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SEED = 123\n",
    "BATCH_SIZE = 16\n",
    "LEARNING_RATE = 2e-5\n",
    "WEIGHT_DECAY = 1e-2\n",
    "EPSILON = 1e-8\n",
    "\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "86b8c415-cc33-453a-978d-9fc3cb488508",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 读取文件，返回文件内容\n",
    "def readfile(filename):\n",
    "    with open(filename, encoding=\"utf-8\") as f:\n",
    "        # 按行进行读取\n",
    "        content = f.readlines()\n",
    "        return content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "35dee727-9097-4cd6-89ae-b69a42744229",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 正负情感语料\n",
    "pos_text, neg_text = readfile('./pos.txt'), readfile('./neg.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0eb3d802-f52d-4a38-84b3-6355f05ca4d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000\n",
      "5000\n",
      "10000\n"
     ]
    }
   ],
   "source": [
    "# 所有语料\n",
    "sentences = pos_text + neg_text\n",
    "print(len(pos_text)) # 5000个正样本\n",
    "print(len(neg_text)) # 5000个负样本\n",
    "print(len(sentences)) # 一共1万样本"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "70edc914-caf1-4b17-8f0b-9942d6c11394",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 1)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 设定标签，positive为1，negative为0\n",
    "pos_targets = np.ones((len(pos_text)))\n",
    "neg_targets = np.zeros((len(neg_text)))\n",
    "# 情感label 拼接到一起，shape = (10000, 1)\n",
    "targets = np.concatenate((pos_targets, neg_targets), axis=0).reshape(-1, 1)   \n",
    "targets.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bf4dac79-e41a-4952-89b5-12562026b193",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10000, 1])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 转换为tensor\n",
    "total_targets = torch.tensor(targets)\n",
    "total_targets.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "181eeed0-ead2-4b96-9cfc-5ed6d9a2b763",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5956bfc5cc964058b9a6c49434b9f52e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/110k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4ac0f5b42d34c3683c4969e10421370",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/29.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "820aa4b83cbd480cbd75a21f607b82e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/269k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cca31ab49de047deb6b890e63cc478be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/624 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "PreTrainedTokenizer(name_or_path='bert-base-chinese', vocab_size=21128, model_max_len=512, is_fast=False, padding_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 从预训练模型中加载bert-base-chinese\n",
    "# [UNK] 特征  [CLS]起始 [SEP]结束\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-chinese', cache_dir=\"/root/bert/transformer_file/\")\n",
    "tokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dc24289a-fa5f-4876-b766-e1b49bf68c28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "不错，下次还考虑入住。交通也方便，在餐厅吃的也不错。\n",
      "\n",
      "['不', '错', '，', '下', '次', '还', '考', '虑', '入', '住', '。', '交', '通', '也', '方', '便', '，', '在', '餐', '厅', '吃', '的', '也', '不', '错', '。']\n",
      "[101, 679, 7231, 8024, 678, 3613, 6820, 5440, 5991, 1057, 857, 511, 769, 6858, 738, 3175, 912, 8024, 1762, 7623, 1324, 1391, 4638, 738, 679, 7231, 511, 102]\n",
      "['[CLS]', '不', '错', '，', '下', '次', '还', '考', '虑', '入', '住', '。', '交', '通', '也', '方', '便', '，', '在', '餐', '厅', '吃', '的', '也', '不', '错', '。', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "print(pos_text[2])\n",
    "# 进行分词\n",
    "print(tokenizer.tokenize(pos_text[2]))\n",
    "# bert编码，会增加起始[CLS] 和 结束[SEP]标记\n",
    "print(tokenizer.encode(pos_text[2]))\n",
    "# 将bert编码转换为 字\n",
    "print(tokenizer.convert_ids_to_tokens(tokenizer.encode(pos_text[2])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d5e0813f-0cd6-4a58-9b7b-0f4a213ac700",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[101, 1762, 102]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 在的编码为1762，开始[CLS]编码为101，结束[SEP]编码为102\n",
    "tokenizer.encode('在')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9c8f1009-4ac5-4173-8ca3-2a44962148fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#将每一句转成数字（大于126做截断，小于126做PADDING，加上首尾两个标识，长度总共等于128）\n",
    "def convert_text_to_token(tokenizer, sentence, limit_size=126):\n",
    "    tokens = tokenizer.encode(sentence[:limit_size])  #直接截断\n",
    "    #补齐（pad的索引号就是0）\n",
    "    if len(tokens) < limit_size + 2:                  \n",
    "        tokens.extend([0] * (limit_size + 2 - len(tokens)))\n",
    "    return tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "58f65c0c-8770-443e-83e1-e00c6414b07b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10000, 128])\n"
     ]
    }
   ],
   "source": [
    "# 对每个句子进行编码\n",
    "input_ids = [convert_text_to_token(tokenizer, x) for x in sentences]\n",
    "# 放到tensor中\n",
    "input_tokens = torch.tensor(input_ids)\n",
    "print(input_tokens.shape) #torch.Size([10000, 128])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "143527cd-c798-40ae-8b41-2d529463dd53",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([  101,  1765,  4415,   855,  5390,   679,  7231,  8024,  7317,   704,\n",
       "         1357,  7474,   511,  2791,  7313,  3683,  6772,  2397,  1112,  8024,\n",
       "         2357,  2229,  1394,  4415,   511,   852,  3221,  7392,  7509,  3126,\n",
       "         3362,  1922,  2345,   749,  8024,  3300,   857,  5042,  3211,  2145,\n",
       "         3404,  4638,  2697,  6230,   511,   707,  3717,  4638,  2791,  7313,\n",
       "         7599,  3250,   679,  7231,  8024,  2523,  5653,  6844,   511,  3517,\n",
       "         6887,  4638,  1765,  3691,  3683,  6772,  5552,  8024,  6656,  1071,\n",
       "          800,  6163,   934,   679,  1469,  6455,   511,  6133,  1041,  4157,\n",
       "         6397,  8182,  2399,   128,  3299,  8149,  3189,  8038,  3315,   782,\n",
       "         3221,   128,  3299,  8123,  3189,  1057,   857,  6421,  6983,  2421,\n",
       "         1920,  2414,  2791,  8024,   817,  3419,   711, 11929,  1039,   511,\n",
       "          102,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_tokens[1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a84d60ab-46d1-430d-a77b-8e0148a87819",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 建立mask\n",
    "def attention_masks(input_ids):\n",
    "    atten_masks = []\n",
    "    for seq in input_ids:\n",
    "        # 如果有编码（>0）即为1, pad为0\n",
    "        seq_mask = [float(x>0) for x in seq]\n",
    "        atten_masks.append(seq_mask)\n",
    "    return atten_masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8435050a-fe2c-4457-ae83-2c14cf698b84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1., 1.,  ..., 1., 1., 1.],\n",
      "        [1., 1., 1.,  ..., 0., 0., 0.],\n",
      "        [1., 1., 1.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [1., 1., 1.,  ..., 0., 0., 0.],\n",
      "        [1., 1., 1.,  ..., 0., 0., 0.],\n",
      "        [1., 1., 1.,  ..., 1., 0., 0.]])\n",
      "torch.Size([10000, 128])\n"
     ]
    }
   ],
   "source": [
    "# 生成attention_masks\n",
    "atten_masks = attention_masks(input_ids)\n",
    "# 将atten_masks放到tensor中\n",
    "attention_tokens = torch.tensor(atten_masks)\n",
    "print(attention_tokens)\n",
    "print(attention_tokens.size())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "374e8a28-8a5d-46ae-ad21-f10456e5787e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_tokens:\n",
      " tensor([[ 101, 6983, 2421,  ..., 3119, 7178,  102],\n",
      "        [ 101, 1765, 4415,  ...,    0,    0,    0],\n",
      "        [ 101,  679, 7231,  ...,    0,    0,    0],\n",
      "        ...,\n",
      "        [ 101, 2769, 2697,  ...,    0,    0,    0],\n",
      "        [ 101, 2791, 7313,  ...,    0,    0,    0],\n",
      "        [ 101, 5439,  782,  ...,  102,    0,    0]])\n",
      "total_targets:\n",
      " tensor([[1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        ...,\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.]], dtype=torch.float64)\n",
      "attention_tokens:\n",
      " tensor([[1., 1., 1.,  ..., 1., 1., 1.],\n",
      "        [1., 1., 1.,  ..., 0., 0., 0.],\n",
      "        [1., 1., 1.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [1., 1., 1.,  ..., 0., 0., 0.],\n",
      "        [1., 1., 1.,  ..., 0., 0., 0.],\n",
      "        [1., 1., 1.,  ..., 1., 0., 0.]])\n",
      "input_tokens:\n",
      " tensor([[ 101, 6983, 2421,  ..., 3119, 7178,  102],\n",
      "        [ 101, 1765, 4415,  ...,    0,    0,    0],\n",
      "        [ 101,  679, 7231,  ...,    0,    0,    0],\n",
      "        ...,\n",
      "        [ 101, 2769, 2697,  ...,    0,    0,    0],\n",
      "        [ 101, 2791, 7313,  ...,    0,    0,    0],\n",
      "        [ 101, 5439,  782,  ...,  102,    0,    0]])\n",
      "torch.Size([10000, 128])\n"
     ]
    }
   ],
   "source": [
    "print('input_tokens:\\n', input_tokens) # shape=[10000, 128]\n",
    "print('total_targets:\\n', total_targets) # shape=[10000, 1]\n",
    "print('attention_tokens:\\n', attention_tokens) # shape=[10000, 128]\n",
    "print('input_tokens:\\n', input_tokens) # shape=[10000, 128]\n",
    "print(input_tokens.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d0aec6c6-66db-43e0-ab90-5c71ad7cd2e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8000, 128]) torch.Size([2000, 128])\n",
      "torch.Size([8000, 128]) torch.Size([2000, 128])\n",
      "tensor([ 101, 6983, 2421, 1922, 5439,  749, 8024, 6392, 3177, 6963, 3191,  749,\n",
      "         102,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0])\n",
      "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.])\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "# 使用random_state固定切分方式，切分 train_inputs, train_labels, train_masks,\n",
    "train_inputs, test_inputs, train_labels, test_labels = train_test_split(input_tokens, total_targets, random_state=2021, test_size=0.2)\n",
    "train_masks, test_masks, _, _ = train_test_split(attention_tokens, input_tokens, random_state=666, test_size=0.2)\n",
    "print(train_inputs.shape, test_inputs.shape)    #torch.Size([8000, 128]) torch.Size([2000, 128])\n",
    "print(train_masks.shape, test_masks.shape)      #torch.Size([8000, 128])和train_inputs形状一样\n",
    "\n",
    "print(train_inputs[0])\n",
    "print(train_masks[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e5097146-c7d7-4423-aeb6-1d7999fff693",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用TensorDataset对tensor进行打包\n",
    "train_data = TensorDataset(train_inputs, train_masks, train_labels)\n",
    "# 无放回地随机采样样本元素\n",
    "train_sampler = RandomSampler(train_data)\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=BATCH_SIZE)\n",
    "\n",
    "test_data = TensorDataset(test_inputs, test_masks, test_labels)\n",
    "test_sampler = SequentialSampler(test_data)\n",
    "test_dataloader = DataLoader(test_data, sampler=test_sampler, batch_size=BATCH_SIZE)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "67b0245f-0d4b-471e-92ee-eeb2ab838860",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 101, 1765, 1770,  ...,    0,    0,    0],\n",
      "        [ 101, 3302, 1218,  ...,    0,    0,    0],\n",
      "        [ 101, 1728,  711,  ...,    0,    0,    0],\n",
      "        ...,\n",
      "        [ 101,  855, 5390,  ...,    0,    0,    0],\n",
      "        [ 101, 2345, 8013,  ...,  100,  102,    0],\n",
      "        [ 101, 2792, 3300,  ...,    0,    0,    0]])\n",
      "tensor([[1., 1., 1.,  ..., 0., 0., 0.],\n",
      "        [1., 1., 1.,  ..., 0., 0., 0.],\n",
      "        [1., 1., 1.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [1., 1., 1.,  ..., 0., 0., 0.],\n",
      "        [1., 1., 1.,  ..., 0., 0., 0.],\n",
      "        [1., 1., 1.,  ..., 0., 0., 0.]])\n",
      "tensor([[1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.]], dtype=torch.float64)\n",
      "torch.Size([16, 128]) torch.Size([16, 128]) torch.Size([16, 1])\n",
      "len(train_dataloader)= 500\n"
     ]
    }
   ],
   "source": [
    "# 查看dataloader内容\n",
    "for i, (train, mask, label) in enumerate(train_dataloader):\n",
    "    #torch.Size([16, 128]) torch.Size([16, 128]) torch.Size([16, 1])\n",
    "    print(train)\n",
    "    print(mask)\n",
    "    print(label)\n",
    "    print(train.shape, mask.shape, label.shape)       \n",
    "    break\n",
    "print('len(train_dataloader)=', len(train_dataloader)) #500\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "38b8f2f7-46ca-41ba-81a5-1ef907fbf9ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff6649566a2d4cfe9788319e655948d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/412M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-chinese were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-chinese and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(21128, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# 加载预训练模型， num_labels表示2个分类，好评和差评\n",
    "model = BertForSequenceClassification.from_pretrained(\"bert-base-chinese\", num_labels = 2)\n",
    "# 使用GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e90b09db-1254-4ab4-b822-47fd79a7f66c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nfrom torch import optim\\n# 定义优化器\\n#optimizer = optim.Adam(model.parameters(), lr=1e-3)\\noptimizer = optim.Adam(model.parameters())\\n'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 定义优化器 AdamW， eps默认就为1e-8（增加分母的数值，用来提高数值稳定性）\n",
    "#optimizer = AdamW(model.parameters(), lr = LEARNING_RATE, eps = EPSILON)\n",
    "no_decay = ['bias', 'LayerNorm.weight']\n",
    "optimizer_grouped_parameters = [\n",
    "        {'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': WEIGHT_DECAY},\n",
    "        {'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "]\n",
    "optimizer = AdamW(optimizer_grouped_parameters, lr = LEARNING_RATE, eps = EPSILON)\n",
    "\"\"\"\n",
    "from torch import optim\n",
    "# 定义优化器\n",
    "#optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7b887ee4-67ae-4c1d-8ab3-4655d472909a",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 2\n",
    "# training steps 的数量: [number of batches] x [number of epochs].\n",
    "total_steps = len(train_dataloader) * epochs\n",
    "\n",
    "# 设计 learning rate scheduler.\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps = 0, num_training_steps = total_steps)\n",
    "\n",
    "\n",
    "# # 模型训练、评估"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0feb2b03-c0b5-49e8-9730-c00e650bd38e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 二分类结果评估\n",
    "def binary_acc(preds, labels):      #preds.shape=(16, 2) labels.shape=torch.Size([16, 1])\n",
    "    # eq里面的两个参数的shape=torch.Size([16]) \n",
    "    correct = torch.eq(torch.max(preds, dim=1)[1], labels.flatten()).float()         \n",
    "    if 0:\n",
    "        print('binary acc ********')\n",
    "        print('preds = ', preds)\n",
    "        print('labels = ', labels)\n",
    "        print('correct = ', correct)\n",
    "    acc = correct.sum().item() / len(correct)\n",
    "    return acc\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c3b4b92d-4dfc-4bd1-b82d-2edfe7b98a98",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import datetime\n",
    "# 时间格式化\n",
    "def format_time(elapsed):    \n",
    "    elapsed_rounded = int(round((elapsed)))    \n",
    "    return str(datetime.timedelta(seconds=elapsed_rounded))   #返回 hh:mm:ss 形式的时间\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1b569500-35fb-4b35-9ed9-b94cf097b859",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer):\n",
    "    # 记录当前时刻\n",
    "    t0 = time.time()\n",
    "    # 统计m每个batch的loss 和 acc\n",
    "    avg_loss, avg_acc = [],[]\n",
    "    \n",
    "    # 开启训练模式\n",
    "    model.train()\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        # 每隔40个batch 输出一下所用时间.\n",
    "        if step % 40 == 0 and not step == 0:\n",
    "            elapsed = format_time(time.time() - t0)\n",
    "            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n",
    "        # 从batch中取数据，并放到GPU中\n",
    "        b_input_ids, b_input_mask, b_labels = batch[0].long().to(device), batch[1].long().to(device), batch[2].long().to(device)\n",
    "        # 前向传播，得到output\n",
    "        output = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask, labels=b_labels)\n",
    "        # 得到loss和预测结果logits\n",
    "        loss, logits = output[0], output[1]\n",
    "        # 记录每次的loss和acc\n",
    "        avg_loss.append(loss.item())\n",
    "        # 评估acc\n",
    "        acc = binary_acc(logits, b_labels)\n",
    "        avg_acc.append(acc)\n",
    "        # 清空上一轮梯度\n",
    "        optimizer.zero_grad()\n",
    "        # 反向传播\n",
    "        loss.backward()\n",
    "        # 大于1的梯度将其设为1.0, 以防梯度爆炸\n",
    "        clip_grad_norm_(model.parameters(), 1.0)\n",
    "        # 更新模型参数\n",
    "        optimizer.step()\n",
    "        #更新learning rate\n",
    "        scheduler.step()\n",
    "    # 统计平均loss和acc\n",
    "    avg_loss = np.array(avg_loss).mean()\n",
    "    avg_acc = np.array(avg_acc).mean()\n",
    "    return avg_loss, avg_acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6913ea65-415f-4826-9fba-b7f80d62983f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 模型评估\n",
    "def evaluate(model):\n",
    "    avg_acc = []\n",
    "    #表示进入测试模式\n",
    "    model.eval()         \n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in test_dataloader:\n",
    "            # 从batch中取数据，并放到GPU中\n",
    "            b_input_ids, b_input_mask, b_labels = batch[0].long().to(device), batch[1].long().to(device), batch[2].long().to(device)\n",
    "            # 前向传播，得到output\n",
    "            output = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)\n",
    "            # 统计当前batch的acc\n",
    "            acc = binary_acc(output[0], b_labels)\n",
    "            avg_acc.append(acc)\n",
    "    # 统计平均acc\n",
    "    avg_acc = np.array(avg_acc).mean()\n",
    "    return avg_acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "22b874de-1d6d-4ffc-a2b0-a6ee41333708",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch    40  of    500.    Elapsed: 0:00:13.\n",
      "  Batch    80  of    500.    Elapsed: 0:00:26.\n",
      "  Batch   120  of    500.    Elapsed: 0:00:40.\n",
      "  Batch   160  of    500.    Elapsed: 0:00:53.\n",
      "  Batch   200  of    500.    Elapsed: 0:01:07.\n",
      "  Batch   240  of    500.    Elapsed: 0:01:20.\n",
      "  Batch   280  of    500.    Elapsed: 0:01:34.\n",
      "  Batch   320  of    500.    Elapsed: 0:01:47.\n",
      "  Batch   360  of    500.    Elapsed: 0:02:01.\n",
      "  Batch   400  of    500.    Elapsed: 0:02:15.\n",
      "  Batch   440  of    500.    Elapsed: 0:02:29.\n",
      "  Batch   480  of    500.    Elapsed: 0:02:43.\n",
      "epoch=0,训练准确率=0.855125，损失=0.33603662432730197\n",
      "epoch=0,测试准确率=0.8985\n",
      "  Batch    40  of    500.    Elapsed: 0:00:14.\n",
      "  Batch    80  of    500.    Elapsed: 0:00:28.\n",
      "  Batch   120  of    500.    Elapsed: 0:00:42.\n",
      "  Batch   160  of    500.    Elapsed: 0:00:56.\n",
      "  Batch   200  of    500.    Elapsed: 0:01:10.\n",
      "  Batch   240  of    500.    Elapsed: 0:01:24.\n",
      "  Batch   280  of    500.    Elapsed: 0:01:38.\n",
      "  Batch   320  of    500.    Elapsed: 0:01:52.\n",
      "  Batch   360  of    500.    Elapsed: 0:02:06.\n",
      "  Batch   400  of    500.    Elapsed: 0:02:20.\n",
      "  Batch   440  of    500.    Elapsed: 0:02:34.\n",
      "  Batch   480  of    500.    Elapsed: 0:02:48.\n",
      "epoch=1,训练准确率=0.934625，损失=0.18824554945901037\n",
      "epoch=1,测试准确率=0.903\n"
     ]
    }
   ],
   "source": [
    "# 训练 & 评估\n",
    "for epoch in range(epochs): \n",
    "    # 模型训练\n",
    "    train_loss, train_acc = train(model, optimizer)\n",
    "    print('epoch={},训练准确率={}，损失={}'.format(epoch, train_acc, train_loss))\n",
    "    # 模型评估\n",
    "    test_acc = evaluate(model)\n",
    "    print(\"epoch={},测试准确率={}\".format(epoch, test_acc))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8349ac42-fe24-40b9-96bc-78464cb62bf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(sen):\n",
    "    # 将sen 转换为id\n",
    "    input_id = convert_text_to_token(tokenizer, sen)\n",
    "    print(input_id)\n",
    "    # 放到tensor中\n",
    "    input_token =  torch.tensor(input_id).long().to(device)            #torch.Size([128])\n",
    "    # 统计有id的部分，即为 1(mask)，并且转换为float类型\n",
    "    atten_mask = [float(i>0) for i in input_id]\n",
    "    # 将mask放到tensor中\n",
    "    attention_token = torch.tensor(atten_mask).long().to(device)       #torch.Size([128])\n",
    "    # 转换格式 size= [1,128]， torch.Size([128])->torch.Size([1, 128])否则会报错\n",
    "    attention_mask = attention_token.view(1, -1)\n",
    "\n",
    "    output = model(input_token.view(1, -1), token_type_ids=None, attention_mask=attention_mask)\n",
    "    return torch.max(output[0], dim=1)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0eaf8066-e466-4b07-978b-11680a387755",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[101, 6983, 2421, 855, 5390, 7410, 2823, 8024, 4384, 1862, 679, 1922, 1962, 8024, 7392, 7509, 2345, 8024, 678, 3613, 679, 833, 1086, 3341, 4638, 511, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "差评\n"
     ]
    }
   ],
   "source": [
    "label = predict('酒店位置难找，环境不太好，隔音差，下次不会再来的。')\n",
    "print('好评' if label==1 else '差评')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1ceb98ef-ad14-4589-944f-1c778895d096",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[101, 6983, 2421, 6820, 1377, 809, 8024, 2970, 2521, 782, 1447, 2523, 4178, 2658, 8024, 1310, 4495, 1394, 3419, 8024, 4958, 7313, 738, 3683, 6772, 1920, 8024, 679, 6639, 4638, 1765, 3175, 2218, 3221, 3766, 3300, 4970, 2787, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "好评\n"
     ]
    }
   ],
   "source": [
    "label = predict('酒店还可以，接待人员很热情，卫生合格，空间也比较大，不足的地方就是没有窗户')\n",
    "print('好评' if label==1 else '差评')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f81864fc-618c-4ad9-b8fa-0054f535ead4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[101, 107, 3302, 1218, 1392, 3175, 7481, 3766, 3300, 679, 1453, 1168, 4638, 1765, 3175, 117, 1392, 3175, 7481, 3766, 3300, 3766, 2682, 1168, 4638, 5301, 5688, 107, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "好评\n"
     ]
    }
   ],
   "source": [
    "label = predict('\"服务各方面没有不周到的地方, 各方面没有没想到的细节\"')\n",
    "print('好评' if label==1 else '差评')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "72344251-dd31-4c92-8bcc-a6b99d589bd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[101, 6983, 2421, 855, 5390, 7410, 2823, 8024, 4384, 1862, 679, 1922, 1962, 8024, 7392, 7509, 2345, 8024, 678, 3613, 679, 833, 1086, 3341, 4638, 511, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "sen = '酒店位置难找，环境不太好，隔音差，下次不会再来的。'\n",
    "input_id = convert_text_to_token(tokenizer, sen)\n",
    "print(input_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ef6ceb8f-31a7-4412-bd79-6dcbd0978dea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 101, 6983, 2421,  855, 5390, 7410, 2823, 8024, 4384, 1862,  679, 1922,\n",
      "        1962, 8024, 7392, 7509, 2345, 8024,  678, 3613,  679,  833, 1086, 3341,\n",
      "        4638,  511,  102,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "input_token =  torch.tensor(input_id).long().to(device)            #torch.Size([128])\n",
    "print(input_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "cde6bde4-5acb-4586-bdb4-7ec8c7d94eea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "atten_mask=\n",
      " [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n"
     ]
    }
   ],
   "source": [
    "# 统计有id的部分，即为 1(mask)，并且转换为float类型\n",
    "atten_mask = [float(i>0) for i in input_id]\n",
    "print('atten_mask=\\n', atten_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "27abaf81-94d7-4b72-a875-28247bad8d22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 128])\n"
     ]
    }
   ],
   "source": [
    "# 将mask放到tensor中\n",
    "attention_token = torch.tensor(atten_mask).long().to(device)       #torch.Size([128])\n",
    "# 转换格式 size= [1,128]\n",
    "attention_mask = attention_token.view(1, -1)\n",
    "print(attention_mask.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "02865a5d-da0e-4b6e-bf19-82d403d99aee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SequenceClassifierOutput(loss=None, logits=tensor([[ 2.6870, -2.6489]], device='cuda:0', grad_fn=<AddmmBackward>), hidden_states=None, attentions=None)\n",
      "tensor([[ 2.6870, -2.6489]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "result= tensor([0], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "output = model(input_token.view(1, -1), token_type_ids=None, attention_mask=attention_mask)     #torch.Size([128])->torch.Size([1, 128])否则会报错\n",
    "print(output)\n",
    "print(output[0])\n",
    "\n",
    "print('result=', torch.max(output[0], dim=1)[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "6e50426b-0425-47bc-ae49-8e7983725611",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[101, 2600, 860, 679, 7231, 8024, 852, 3221, 671, 3517, 1555, 2421, 1912, 1259, 1400, 8024, 1555, 1501, 6574, 7030, 679, 5543, 924, 6395, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "好评\n"
     ]
    }
   ],
   "source": [
    "label = predict('总体不错，但是一楼商店外包后，商品质量不能保证')\n",
    "print('好评' if label==1 else '差评')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99247c12-879f-4ec2-bafe-445ae3244161",
   "metadata": {},
   "outputs": [],
   "source": [
    "label = predict('风光秀丽')\n",
    "print('好评' if label==1 else '差评')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
